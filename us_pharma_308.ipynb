{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This code for scraping company website and other information. To do the work I have to change few times, so some of code might not work properly.  At the end I had to add some company information manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodecsv as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.drugs.com/pharmaceutical-companies.html'\n",
    "response = get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    link = a['href']\n",
    "    links.append(link)\n",
    "#print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links[62:-26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('your_file.txt', 'w') as f:\n",
    "#     for item in links:\n",
    "#         link = 'https://www.drugs.com'+ item\n",
    "        \n",
    "#         f.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.strip('\\n') for line in open('your_file.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('us_pharma_02.csv', 'ab')\n",
    "writer = csv.writer(csv_file, dialect='excel', delimiter=',', encoding='utf-8')\n",
    "#writer.writerow(['Search_Term', 'Result', 'Title', 'URL', 'Abstract'])\n",
    "writer.writerow(['company_name','city', 'state', 'full_address', 'zip_code', 'website', 'career'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_not_working = []\n",
    "for website in lines:\n",
    "    try:\n",
    "        \n",
    "        url_details = website\n",
    "        response_details = get(url_details)\n",
    "\n",
    "        soup_details = BeautifulSoup(response_details.text, 'html.parser')\n",
    "\n",
    "        details = soup_details.find('p', class_ = 'mgt-2')\n",
    "        address = details.find_all(text = True)\n",
    "        # comapny name\n",
    "        company = soup_details.find('div', class_ = 'content')\n",
    "        company = company.find(\"h1\").text\n",
    "\n",
    "        # street \n",
    "        street = address[1]\n",
    "\n",
    "        # city\n",
    "        city_r = address[2].split('\\r\\n')[1]\n",
    "        city = city_r.split(',')[0]\n",
    "\n",
    "        #state\n",
    "        state = city_r.split(',')[1].strip().split(' ')[0]\n",
    "\n",
    "        # zip code\n",
    "        zip_code = city_r.split(',')[1].strip().split(' ')[1]\n",
    "\n",
    "        # carrer website\n",
    "        career = address[-1]\n",
    "        # full address\n",
    "\n",
    "        address_full = street+ \" \" + city + \",\"+ \" \"+  state + \" \"+zip_code\n",
    "\n",
    "        # company website\n",
    "        web_company = details.a['href']\n",
    "        writer.writerow([company, city, state, address_full, zip_code, web_company, career])\n",
    "        \n",
    "    except:\n",
    "        list_of_not_working.append(website)\n",
    "        pass\n",
    "\n",
    "\n",
    "        \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_not_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.drugs.com/pharmaceutical-companies.html'\n",
    "# response = get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup_mute = BeautifulSoup(response.text, 'html.parser')\n",
    "# mute_list = soup_mute.find_all('a', class_ = \"text-color-muted text-weight-normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_list = []\n",
    "# for i in mute_list:\n",
    "#     link = i['href']\n",
    "#     web= 'https://www.drugs.com'+ link\n",
    "#     remove_list.append(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(remove_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_to_work = []\n",
    "# for i in list_of_not_working:\n",
    "#     if i not in remove_list:\n",
    "#         need_to_work.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('need_to_work.txt', 'w') as f:\n",
    "#     for item in need_to_work:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_need_work = [line.strip('\\n') for line in open('need_to_work.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('not_working.csv', 'ab')\n",
    "writer = csv.writer(csv_file, dialect='excel', delimiter=',', encoding='utf-8')\n",
    "#writer.writerow(['Search_Term', 'Result', 'Title', 'URL', 'Abstract'])\n",
    "writer.writerow(['comapnay','city', 'state', 'full_address', 'zip_code', 'website'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "again_not= []\n",
    "# for website in lines_need_work:\n",
    "    \n",
    "#     try:\n",
    "    \n",
    "# #         print(website)\n",
    "#         url_details = website\n",
    "#         response_details = get(url_details)\n",
    "\n",
    "#         soup_details = BeautifulSoup(response_details.text, 'html.parser')\n",
    "\n",
    "#         details = soup_details.find('p', class_ = 'mgt-2')\n",
    "#         address = details.find_all(text = True)\n",
    "#         # company\n",
    "#         company = address[0]\n",
    "\n",
    "       \n",
    "#         # street 1\n",
    "#         street_1 = address[1]\n",
    "#         street_2 = address[2].split('\\r\\n')[1]\n",
    "#         street = street_1 + \", \" + \" \" + street_2\n",
    "\n",
    "     \n",
    "        \n",
    "#         city_r = address[3].split('\\r\\n')[1]\n",
    "#         city = city_r.split(',')[0]\n",
    "\n",
    "       \n",
    "\n",
    "#         #state\n",
    "#         state = city_r.split(',')[1].strip().split(' ')[0]\n",
    "       \n",
    "\n",
    "#         # zip code\n",
    "#         zip_code = city_r.split(',')[1].strip().split(' ')[1]\n",
    "\n",
    "#         address_full = street+ \" \" + city + \",\"+ \" \"+  state + \" \"+zip_code\n",
    "#         web_company = details.a['href']\n",
    "        \n",
    "#         writer.writerow([company, city, state, address_full, zip_code, web_company])\n",
    "        \n",
    "        \n",
    "#     except:\n",
    "#         again_not.append(website)\n",
    "#         pass\n",
    "\n",
    "# csv_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(again_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('again_not.txt', 'w') as f:\n",
    "    for item in again_not:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "again_not = [line.strip('\\n') for line in open('again_not.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for website in again_not:\n",
    "     \n",
    "    url_details = website\n",
    "    response_details = get(url_details)\n",
    "\n",
    "    soup_details = BeautifulSoup(response_details.text, 'html.parser')\n",
    "\n",
    "    details = soup_details.find('p', class_ = 'mgt-2')\n",
    "    address = details.find_all(text = True)\n",
    "#     pp.pprint(address)\n",
    "#     company = soup_details.find('div', class_ = 'content')\n",
    "#     company = company.find(\"h1\").text\n",
    "# #     print(company)\n",
    "\n",
    "       \n",
    "#     #street 1\n",
    "#     street_1 = address[1]\n",
    "#     street_2 = address[2].split('\\r\\n')[1]\n",
    "#     street = street_1 + \", \" + \" \" + street_2\n",
    "\n",
    "#     print(street)\n",
    "\n",
    "\n",
    "    city_r = address[3].split('\\r\\n')[1]\n",
    "    city = city_r.split(',')[0]\n",
    "\n",
    "    print(city)\n",
    "\n",
    "#     #state\n",
    "#     state = city_r.split(',')[1].strip().split(' ')[0]\n",
    "\n",
    "\n",
    "#     # zip code\n",
    "#      zip_code = city_r.split(',')[1].strip().split(' ')[1]\n",
    "\n",
    "#     address_full = street+ \" \" + city + \",\"+ \" \"+  state + \" \"+zip_code\n",
    "   \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('list_of_not_working_again.csv', 'ab')\n",
    "writer = csv.writer(csv_file, dialect='excel', delimiter=',', encoding='utf-8')\n",
    "#writer.writerow(['Search_Term', 'Result', 'Title', 'URL', 'Abstract'])\n",
    "# writer.writerow(['comapnay','city', 'state', 'full_address', 'zip_code'])\n",
    "writer.writerow(['compnay','street'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_not_working_again = []\n",
    "for website in again_not:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        url_details = website\n",
    "        response_details = get(url_details)\n",
    "\n",
    "        soup_details = BeautifulSoup(response_details.text, 'html.parser')\n",
    "\n",
    "        details = soup_details.find('p', class_ = 'mgt-2')\n",
    "        address = details.find_all(text = True)\n",
    "        # comapny name\n",
    "\n",
    "#         pp.pprint(address)\n",
    "        company = soup_details.find('div', class_ = 'content')\n",
    "        company = company.find(\"h1\").text\n",
    "#         pp.pprint(company)\n",
    "            \n",
    "#                 #street 1\n",
    "        street_1 = address[1]\n",
    "        street_2 = address[2].split('\\r\\n')[1]\n",
    "        street = street_1 + \", \" + \" \" + street_2\n",
    "\n",
    "#         pp.pprint(street)\n",
    "\n",
    "\n",
    "#         city_r = address[3].split('\\r\\n')[1]\n",
    "#         city = city_r.split(',')[0]\n",
    "# #         pp.pprint(city)\n",
    "\n",
    "\n",
    "\n",
    "#         #state\n",
    "#         state = city_r.split(',')[1].strip().split(' ')[0]\n",
    "#         pp.pprint(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # zip code\n",
    "#         zip_code = city_r.split(',')[1].strip().split(' ')[1]\n",
    "#         pp.pprint(zip_code)\n",
    "        \n",
    "#         pp.pprint(\"{{{{{{{{{{{{{{{{{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}\")\n",
    "\n",
    "#         address_full = street+ \" \" + city + \",\"+ \" \"+  state + \" \"+zip_code\n",
    "   \n",
    "        writer.writerow([company,street])\n",
    "        \n",
    "    except:\n",
    "        list_of_not_working_again.append(website)\n",
    "#         print(website)\n",
    "        pass\n",
    "\n",
    "\n",
    "        \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
